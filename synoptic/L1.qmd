---
title: "L1"
author: "COMPASS workflows team"
title-block-banner: true
params:
  html_outfile: "L1.html"
  DATA_ROOT: "data_TEST/"
  L1_NORMALIZE: "L1_normalize/"
  L1: "L1/"
  METADATA_ROOT: "metadata/"
  L1_METADATA: "L1_metadata/"
  METADATA_VARS_TABLE: "L1_metadata_variables.csv"
  METADATA_COLUMNS_TABLE: "L1_metadata_columns.csv"
  L1_VERSION: "???"
  write_plots: true
  logfile: ""
  run_parallel: false
date: now
date-format: "YYYY-MM-DD HH:mm:ssZ"
format: 
  html:
    code-fold: true
editor: visual
---

This script

-   Reads in all the L1_normalize files

-   Compiles and writes them out into separate \<site\>\<year\>\<month\> files

-   Writes a metadata file for each folder

## Initializing

```{r init}
#| include: false

library(ggplot2)
theme_set(theme_bw())
library(compasstools)
if(packageVersion("compasstools") < "0.2") {
    stop("Please update to latest version of compasstools!\n",
         "devtools::install_github('COMPASS-DOE/compasstools')")
}

source("helpers.R")

# Get the column metadata file
column_md <- read.csv(file.path(params$METADATA_ROOT,
                                params$L1_METADATA,
                                params$METADATA_COLUMNS_TABLE),
                      comment.char = "#",
                      stringsAsFactors = FALSE)

# Open the fLag database (in the data, not metadata, folder)
#source("flag-database.R")
#fdb_open(params$DATA_ROOT, init = TRUE)

L1_NORMALIZE <- file.path(params$DATA_ROOT, params$L1_NORMALIZE)
dirs_to_process <- scan_folders(L1_NORMALIZE)

L1 <- file.path(params$DATA_ROOT, params$L1)
```

I see `r length(dirs_to_process)` directories to process in `r L1_NORMALIZE`.

L1 column metadata has `r nrow(column_md)` entries.

Output directory is `r L1`.

HTML outfile is "`r params$html_outfile`".

## Processing

```{r processing}
f <- function(dir_name, dirs_to_process, out_dir) {
    message(Sys.time(), " Processing ", basename(dir_name))
    d <- dirs_to_process[[dir_name]]
    message("\tIt has ", length(d), " files")
    
    # Read all files in a folder
    dat_raw <- read_csv_group(d, col_types = "cccccTdccccccccdii")
    
    message("\tTotal data: ", nrow(dat_raw), " rows, ", ncol(dat_raw), " columns")
    
    # Remove duplicate rows (e.g. from multiple datalogger downloads)
    dat <- dat_raw[!duplicated(dat_raw), ]
    message("\tRemoved ", nrow(dat_raw) - nrow(dat), " duplicate rows")
    
    # File-based summary
    smry <- data.frame(Dir = dir_name, 
                       Files = length(d), 
                       Rows = nrow(dat),
                       NA_rows = sum(is.na(dat$Value)))
    
    if(nrow(dat)) {
        site <- dat$Site[1]
        plot <- dat$Plot[1]
        
        # Check for metadata columns that are missing...
        if(!all(column_md$Column %in% colnames(dat))) {
            stop("Column metadata file ", params$METADATA_COLUMNS_TABLE,
                 " has entries not in data: ", setdiff(column_md$Column, colnames(dat)))
        }
        # ...order and remove columns not in the metadata...
        dat <- dat[column_md$Column]
        # ...and sort rows
        dat <- dat[order(dat$TIMESTAMP),]
        
        write_to_folders(dat, 
                         root_dir = out_dir, 
                         data_level = "L1",
                         site = site,
                         plot = plot,
                         version = params$L1_VERSION,
                         write_plots = params$write_plots)
        
        # Add any OOB flags to the flag database
        # flg <- dat[!is.na(dat$F_OOB) & dat$F_OOB == 1, "ID"]
        # if(nrow(flg)) {
        #     flg$Flag_type <- "OOB"
        #     flg$Remark <- basename(dir_name)
        #     message("Sending ", nrow(flg), " flags to database")
        #     fdb_add_flags(site, flg)
        # }
    }
    rm(dat)
    return(smry)
}

# We can optionally process in parallel using all available cores
if(params$run_parallel) {
    library(parallel)
    apply_func = mclapply
    # L1 is memory-intensive, especially for the TEMPEST files, and 
    # it's possible to run out of system memory, so limit cores used
    options(mc.cores = min(6, detectCores()))
    message("Using parallel::mclapply with ", getOption("mc.cores"), " cores")
} else {
    apply_func = lapply
}

log_info("About to L1", logfile = params$logfile)
tryCatch({
    out <- apply_func(names(dirs_to_process), f, 
                      dirs_to_process = dirs_to_process, out_dir = L1)
},
error = function(e) {
    log_warning("L1: an error occurred!", logfile = params$logfile)
    log_info(as.character(e), logfile = params$logfile)
    stop(e)
})

out_df <- do.call("rbind", out)
n_obs <- format(sum(out_df$Rows), big.mark = ",")
n_na <- format(sum(out_df$NA_rows), big.mark = ",")
```

## Metadata

L1 metadata template directory is `r params$L1_METADATA`.

```{r metadata}
# Write the overall README
readme_fn <- file.path(params$METADATA_ROOT, params$L1_METADATA, 
                       paste0("README_v", params$L1_VERSION, ".txt"))
if(!file.exists(readme_fn)) stop("Couln't find ", readme_fn)
readme <- readLines(readme_fn)
readme <- gsub("[VERSION]", params$L1_VERSION, readme, fixed = TRUE)
readme <- gsub("[DATESTAMP]", format(Sys.time(), "%Y-%m-%d"), readme, fixed = TRUE)
readme <- gsub("[OBSERVATIONS]", n_obs, readme, fixed = TRUE)
readme <- gsub("[GIT_COMMIT]", GIT_COMMIT, readme, fixed = TRUE)
readme_outfn <- file.path(L1, basename(readme_fn))
message("Writing overall README ", readme_outfn, "...")
writeLines(readme, readme_outfn)

# Get the L1 template file
template_file <- file.path(params$METADATA_ROOT,
                           params$L1_METADATA, 
                           "L1_metadata_template.txt")
if(!file.exists(template_file)) {
    stop("Couldn't find file ", basename(template_file), " in ", params$L1_METADATA)
}
L1_metadata_template <- readLines(template_file)

col_md_for_insert <- paste(sprintf("%-15s", column_md$Column), column_md$Description)

# Get the variable metadata
var_md <- read.csv(file.path(params$METADATA_ROOT, params$L1_METADATA, params$METADATA_VARS_TABLE))
var_md_for_insert <- paste(sprintf("%-20s", c("research_name", var_md$research_name)),
                           sprintf("%-12s", c("Sensor", var_md$sensor)),
                           sprintf("%-10s", c("Units", var_md$final_units)),
                           sprintf("%-12s", c("Bounds", paste0(var_md$low_bound, ", ", var_md$high_bound))),
                           c("Description", var_md$description))

message("Main template has ", length(L1_metadata_template), " lines")
message("Column metadata info has ", length(col_md_for_insert), " lines")
message("Variable metadata info has ", length(var_md_for_insert), " lines")

message("Inserting data version string")
L1_metadata_template <- gsub("[VERSION]", params$L1_VERSION,
                             L1_metadata_template, fixed = TRUE)

# Identify the main data directories in L1/{version}/, which are <site>_<year>
data_dirs <- list.files(L1, pattern = "^[a-zA-Z]+_[0-4]{4}$")

for(dd in data_dirs) {
    dd_full <- file.path(L1, dd)
    message("Generating metadata for ", dd_full)
    
    message("\tInserting verion")
    md <- gsub("[VERSION]", params$L1_VERSION, L1_metadata_template, fixed = TRUE)
    
    message("\tInserting timestamp and folder name")
    md <- gsub("[TIMESTAMP]", date(), md, fixed = TRUE)
    md <- gsub("[FOLDER_NAME]", dd, md, fixed = TRUE)
    
    # File info
    files <- list.files(path = dd_full, pattern = "csv$", full.names = TRUE)
    message("\tFound ", length(files), " data files")
    file_info <- c()
    # Build up information about files...
    for(f in files) {
        fdata <- readLines(f) # just for a quick line count
        file_info <- c(file_info, 
                       basename(f),
                       paste("Rows:", length(fdata) - 1),
                       paste("md5:", digest::digest(f, file = TRUE)),
                       "")
    }
    # ...and insert into metadata
    # We used the head(-1) to drop the final empty line, just to keep things pretty
    file_info_pos <- grep("[FILE_INFO", md, fixed = TRUE)
    md <- append(md, head(file_info, -1), after = file_info_pos)
    md <- md[-file_info_pos]
    
    # Insert column metadata
    col_info_pos <- grep("[COLUMN_INFO]", md, fixed = TRUE)
    md <- append(md, col_md_for_insert, after = col_info_pos)
    md <- md[-col_info_pos]
    # The NA code is an in-line replacement
    md <- gsub("[NA_STRING_L1]", NA_STRING_L1, md, fixed = TRUE)

    # Insert variable metadata
    var_info_pos <- grep("[VARIABLE_INFO]", md, fixed = TRUE)
    md <- append(md, var_md_for_insert, after = var_info_pos)
    md <- md[-var_info_pos]
    
    # Site information
    # Folders are <site>_<year>
    # There MUST be a informational file named <site>.txt
    site <- strsplit(dd, "_")[[1]][1]
    site_md_file <- file.path(params$METADATA_ROOT, params$L1_METADATA, paste0(site, ".txt"))
    if(!file.exists(site_md_file)) {
        stop("Couldn't find file ", site_md_file, " in ", params$L1_METADATA)
    }
    site_md_for_insert <- readLines(site_md_file)

    # Insert site information    
    site_info_pos <- grep("[SITE_INFO]", md, fixed = TRUE)
    md <- append(md, site_md_for_insert, after = site_info_pos)
    md <- md[-site_info_pos]

    # Write the final metadata file
    mdfn <- paste0(dd, "_L1_v", params$L1_VERSION, "_metadata.txt")
    message("\tWriting ", mdfn, "...")
    writeLines(md, file.path(L1, dd, mdfn))
}
```

## Output summary

```{r output_summary_table}
knitr::kable(out_df)
```

Total rows written: `r n_obs`

Total NA rows written: `r n_na`

## Clean up

```{r cleanup}
#fdb_cleanup() # Close flags database
```

## Reproducibility

Git commit `r GIT_COMMIT`.

```{r reproducibility}
sessionInfo()
```
