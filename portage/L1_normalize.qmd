---
title: "L1_normalize"
author: "COMPASS workflows team"
title-block-banner: true
params:
  L0: "L0/"
  html_outfile: "L1_normalize.html"
  L1_normalize: "L1_normalize/"
  design_table: "design_table.csv"
date: now
date-format: "YYYY-MM-DD HH:mm:ssZ"
format: 
  html:
    code-fold: true
editor: visual
---

## Initializing

```{r}
#| output: false
library(tidyr)
library(dplyr)
library(readr)
library(compasstools)
if(!exists("expand_df")) {
    stop("Please update to latest version of compasstools!\n",
         "devtools::install_github('COMPASS-DOE/compasstools')")
}
dt <- read_csv(params$design_table)
dt_ex <- expand_df(dt)

files_to_process <- list.files(params$L0, pattern = "*.csv$", full.names = TRUE)
```

I see `r length(files_to_process)` files to process.

Design table is `r nrow(dt)` rows, `r nrow(dt_ex)` after expansion.

HTML outfile is "`r params$html_outfile`".

## Processing

```{r}
f <- function(fn, out_dir, design_table) {
    message(Sys.time(), " Processing ", basename(fn))
    dat <- read_csv(fn, 
                    # don't want timestamp parsed to a datetime at this point
                    col_types = list(TIMESTAMP = col_character()))
    
    dat_long <- pivot_longer(dat, c(-Logger, -Table, -TIMESTAMP),
                             names_to = "loggernet_variable",
                             values_to = "value",
                             # We're stacking both numeric and character columns, 
                             # so force everything to character (no precision 
                             # loss since data came from a text file anyway)
                             values_transform = as.character)
    message("\tPivoted data has ", nrow(dat_long), " rows")
    dat_long <- filter(dat_long, !is.na(value))
    message("\tFiltered data has ", nrow(dat_long), " rows")
    
    # Join with design table
    nrow_orig <- nrow(dat_long)
    dat_long <- left_join(dat_long, design_table,
                          by = c("Logger", "Table", "loggernet_variable"))
    
    # Compute the top three loggernet variables with missing design links
    dat_long %>% 
        filter(is.na(design_link)) %>% 
        group_by(loggernet_variable) %>% 
        summarise(N = n()) %>% 
        arrange(desc(N)) %>% 
        slice_head(n = 3) %>% 
        pull(loggernet_variable) ->
        top3_na_vars
    
    smry <- data.frame(File = basename(fn), 
                       Pivoted_rows = nrow_orig,
                       Joined_rows = nrow(dat_long),
                       dl_NA = sum(is.na(dat_long$design_link)),
                       dl_NA_top3 = paste(top3_na_vars, collapse = ", "))

    message("\tFiltering out missing design_link rows")
    dat_long <- filter(dat_long, !is.na(design_link))
    
    new_fn <- gsub("L0\\.csv$", "L1_norm\\.csv", basename(fn))
    
    outfile <- file.path(out_dir, new_fn)
    message("\tWriting ", outfile)
    x <- try(write_csv(dat_long, outfile))
    if(is.data.frame(x)) {
        file.remove(fn)
    } else {
        warning("Write error!")
    }
    return(smry)
}

out <- lapply(files_to_process, f, 
              out_dir = params$L1_normalize,
              design_table = dt_ex)
```

## Summary

```{r}
out_df <- do.call("rbind", out)
knitr::kable(out_df)
```

## Reproducibility

```{r}
sessionInfo()
```
