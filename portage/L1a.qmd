---
title: "L1a"
author: "COMPASS workflows team"
title-block-banner: true
params:
  L1_normalize: "data_TEST/L1_normalize/"
  remove_input_files: false
  L1a: "data_TEST/L1a/"
  plot_table: "data_TEST/plot_table.csv"
  units_table: "data_TEST/units_table.csv"
  bounds_table: "data_TEST/bounds_table.csv"
  newvars_table: "data_TEST/newvars_table.csv"
  timestamp_round: "15 minutes"
  html_outfile: "L1a.html"
  logfile: ""
date: now
date-format: "YYYY-MM-DD HH:mm:ssZ"
format: 
  html:
    code-fold: true
editor: visual
---

This script

-   Reads in all the L1_normalize files by year/month/site

-   Performs unit conversion

-   Performs bounds checking (adding flags) for each \`research_name\` variable

-   Rounds timestsamps

-   Computes new variables: `sapflow_avg` (a test)

-   Joins with plot table

## Initializing

```{r}
#| include: false

library(tidyr)
library(dplyr)
library(readr)
library(lubridate)
library(compasstools)
if(!exists("scan_folders") | !exists("unit_conversion")) {
    stop("Please update to latest version of compasstools!\n",
         "devtools::install_github('COMPASS-DOE/compasstools')")
}
pt <- read_csv(params$plot_table, col_types = "cccc")
pt_ex <- expand_df(pt)

ut <- read_csv(params$units_table, col_types = "ccccccc")
ut <- filter(ut, !is.na(research_name))

read_csv(params$bounds_table, col_types = "cccccc") %>% 
    filter(!is.na(research_name), !is.na(units)) %>% 
    select(research_name, units, low, high) %>% 
    mutate(low = as.numeric(low), high = as.numeric(high)) ->
    bt

nvt <- read_csv(params$newvars_table, col_types = "cc")

source("helpers.R")

dirs_to_process <- scan_folders(params$L1_normalize)
```

I see `r length(dirs_to_process)` directories to process.

Plot table is `r nrow(pt)` rows, `r nrow(pt_ex)` after expansion.

Units table is "`r params$units_table`" and has `r nrow(ut)` rows with conversion strings.

Bounds table is "`r params$bounds_table`" and has `r nrow(bt)` rows with bounds.

New variables table is "`r params$newvars_table`" and has `r nrow(nvt)` rows.

HTML outfile is "`r params$html_outfile`".

## Processing

```{r newvar_code}
#| include: false

# This chunk holds dedicated functions to compute new variables
# These must be named "NEWVAR_[new_research_name]", where "new_research_name"
# is given in the new variables CSV files
# Functions must return the data frame they're given, with "value" and "units" 
# columns added (optionally, also a "OOB" column)

# Compute sapflow_avg, the average of shallow and deep sapflow values
NEWVAR_sapflow_avg <- function(x) {
    x$value <- (x$sapflow + x$sapflow_deep) / 2.0
    x$units <- "??"
    return(x)
}
```

```{r}
overwrites <- 0
errors <- 0
dat_issues <- 0
file_read_smry <- data.frame()

f <- function(dir_name, dirs_to_process, out_dir) {
    message(Sys.time(), " Processing ", basename(dir_name))
    message("\tIt has ", length(dirs_to_process[[dir_name]]), " files")
    
    dat <- read_csv_group(dirs_to_process[[dir_name]],
                          remove_input_files = params$remove_input_files, 
                          col_types = "ccTcdcccic")
    errors <<- errors + attr(dat, "errors")
    
    # File-based summary
    message("\tTotal data: ", nrow(dat), " rows, ", ncol(dat), " columns")
    smry <- data.frame(Dir = dir_name, Files = length(dirs_to_process[[dir_name]]), 
                       Rows = nrow(dat), note = "")
    file_read_smry <<- rbind(file_read_smry, smry)
    
    message("\tDoing unit conversion")
    dat <- compasstools::unit_conversion(dat, ut)
    dat <- rename(dat, value_raw = value, value = value_conv)
    dat$value_conv <- NULL
    
    # Create out-of-bounds flags
    message("\tAdding OOB flags")
    dat %>% 
        left_join(bt, by = c("research_name", "units")) %>% 
        mutate(OOB = value < low | value > high) %>% 
        select(-low, -high) ->
        dat
    
    # Round timestamps
    message("\tRounding timestamps to nearest ", params$timestamp_round)
    dat$TIMESTAMP <- round_date(dat$TIMESTAMP, params$timestamp_round)
    
    message("\tJoining with plot table")
    dat %>% 
        left_join(pt_ex, by = c("Site", "design_link")) %>% 
        select(-Table, -loggernet_variable) ->
        dat_mrg

    # Compute new (derived) variables
    message("\tComputing derived variables")
    for(i in seq_len(nrow(nvt))) {
        new_rn <- nvt$new_research_name[i]
        rn_needed <- strsplit(nvt$needs_research_names[i], ",", fixed = TRUE)[[1]]
        # Isolate needed data and site/plot/tree/timestamp
        dat_mrg %>% 
            filter(research_name %in% rn_needed) %>% 
            select(Site, Plot, Tree, TIMESTAMP, research_name, value) %>% 
            pivot_wider(names_from = "research_name", values_from = "value") ->
            dat_subset

        message("\t\t", new_rn, " <- ", nvt$needs_research_names[i],
                " (using ", nrow(dat_subset), " rows)")
        
        # Call the computation function based on the name of the new variable
        # New variable functions MUST add "value" and "units" columns; optionally "OOB"
        dat_subset <- do.call(paste0("NEWVAR_", new_rn), list(dat_subset))
        # Clean up: add research_name...
        dat_subset$research_name <- new_rn
        # remove old research name columns (from the pivot_wider above)
        dat_subset[rn_needed] <- NULL
        # ...and add information entries in the design_link and Logger columns 
        if("design_link" %in% names(dat_mrg)) dat_subset$design_link <- "<derived>"
        if("Logger" %in% names(dat_mrg)) dat_subset$Logger <- "<derived>"
        # This is inefficient but assuming we won't have many
        # Also, new variables might be needed by further ones?!?
        dat_mrg <- bind_rows(dat_mrg, dat_subset)
    }
 
    # Plot table-based summary
    dat_mrg %>% 
        group_by(Site, Plot) %>% 
        summarise(TS_min = min(TIMESTAMP), 
                  TS_max = max(TIMESTAMP), 
                  Vars = length(unique(research_name)),
                  OOB = sum(OOB, na.rm = TRUE),
                  .groups = "drop") %>% 
        mutate(Dir = basename(dir_name), 
               Note = "") %>%
        select(Dir, Site, Plot, Vars, OOB, TS_min, TS_max, Note) ->
        smry

    # Error check
    # There should be only one research_name for each timestamp/site/plot/tree
    dat_mrg %>% 
        group_by(TIMESTAMP, Site, Plot, Tree, research_name) %>% 
        summarise(n = n(), .groups = "drop") %>% 
        filter(n > 1) %>% 
        mutate(Dir = basename(dir_name)) ->
        dat_mrg_problems
    
    if(nrow(dat_mrg_problems)) {
        dat_issues <<- dat_issues + 1
        message("There are duplicate research_names associated with each site/plot/tree")
        dat_mrg_problems$TIMESTAMP <- NULL
        print(distinct(dat_mrg_problems))
        smry$Note <- "Problems!"
    }
    
    # Put columns in Site, Plot, Tree, TIMESTAMP_R, research_name, value,
    # and then <others alphabetized> order
    lefts <- c("Site", "Plot", "Tree", "TIMESTAMP", "research_name", "value")
    dat_mrg <- dat_mrg[c(lefts, sort(setdiff(names(dat_mrg), lefts)))]
    
    write_to_folders(dat_mrg, root_dir = out_dir, data_level = "L1a",
                     site = dat$Site[1], logger = dat$Logger[1], table = "")
    
    return(smry)
}

out <- lapply(names(dirs_to_process), f, 
              dirs_to_process = dirs_to_process, out_dir = params$L1a)
```

## File summary

```{r}
#| echo: false
#| output: asis
if(overwrites) {
    cat("### WARNING: ", overwrites, " file overwrite(s)\n")
    log_warning(paste("File overwrite(s)", params$html_outfile), 
                logfile = params$logfile)
}
if(errors) {
    cat("### WARNING: ", errors, " file read/write error(s)\n")
    log_warning(paste("File read/write error(s)", params$html_outfile), 
                logfile = params$logfile)
}
if(dat_issues) {
    cat("### WARNING: ", dat_issues, " data structure issue(s)\n")
    log_warning(paste("data structure issue(s)", params$html_outfile), 
                logfile = params$logfile)
}
```

```{r}
knitr::kable(file_read_smry)
```

## Plot summary:

```{r}
out_df <- do.call("rbind", out)
knitr::kable(out_df)
```

## Reproducibility

```{r}
sessionInfo()
```
