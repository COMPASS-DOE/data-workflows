---
title: "L1a"
author: "COMPASS workflows team"
title-block-banner: true
params:
  html_outfile: "L1a.html"
  DATA_ROOT: "data_TEST/"
  L1_NORMALIZE: "L1_normalize/"
  L1A: "L1a/"
  OUTPUT_TEMPLATES: "L1a_output_templates/"
  NEWVARS_TABLE: "newvars_table.csv"
  debug: false
  remove_input_files: false
  logfile: ""
date: now
date-format: "YYYY-MM-DD HH:mm:ssZ"
format: 
  html:
    code-fold: true
editor: visual
---

This script

-   Reads in all the L1_normalize files by year/month/site

-   Computes new variables: `sapflow_avg` (a test)

-   Loads all template tables and generates L1a output tables from them

## Initializing

```{r init}
#| include: false

library(tidyr)
library(dplyr)
library(readr)
library(lubridate)
library(compasstools)
if(!exists("scan_folders") | !exists("unit_conversion")) {
    stop("Please update to latest version of compasstools!\n",
         "devtools::install_github('COMPASS-DOE/compasstools')")
}

# The template tables in $OUTPUT_TEMPLATES are 'plot tables' in RR's terminology
# Read them into a list
ot <- list.files(file.path(params$DATA_ROOT, params$OUTPUT_TEMPLATES),
                 pattern = "*.csv$", full.names = TRUE)
templates <- lapply(ot, read_csv, show_col_types = FALSE)
names(templates) <- gsub("\\.csv", "", basename(ot))
# For compactness, plot tables may have expansions
templates <- lapply(templates, compasstools::expand_df)

# Read new variables table
NEWVARS_TABLE <- file.path(params$DATA_ROOT, params$NEWVARS_TABLE)
nvt <- read_csv(NEWVARS_TABLE, col_types = "cc")

source("helpers.R")

L1_NORMALIZE <- file.path(params$DATA_ROOT, params$L1_NORMALIZE)
dirs_to_process <- scan_folders(L1_NORMALIZE)

L1A <- file.path(params$DATA_ROOT, params$L1A)

```

I see `r length(dirs_to_process)` directories to process in `r L1_NORMALIZE`.

Output directory is `r L1A`.

Output table templates are in `r params$OUTPUT_TEMPLATES`; there are `r length(templates)` of them.

HTML outfile is "`r params$html_outfile`".

## Processing

```{r processing}
errors <- 0

f <- function(dir_name, dirs_to_process, out_dir) {
    message(Sys.time(), " Processing ", basename(dir_name))
    d <- dirs_to_process[[dir_name]]
    message("\tIt has ", length(d), " files")
    
    dat_raw <- read_csv_group(d,
                              remove_input_files = params$remove_input_files, 
                              col_types = "ccTcdcccdcl")
    errors <<- errors + attr(dat_raw, "errors")
    
    # File-based summary
    message("\tTotal data: ", nrow(dat_raw), " rows, ", ncol(dat_raw), " columns")
    smry <- data.frame(Dir = dir_name, 
                       Files = length(d), 
                       Rows = nrow(dat_raw))
    
    # Remove duplicate rows (e.g. from multiple datalogger downloads)
    dat <- distinct(dat_raw)
    message("\tRemoved ", nrow(dat_raw) - nrow(dat), " duplicate rows")    
    
    write_to_folders(dat, 
                     root_dir = out_dir, 
                     data_level = "L1a",
                     site = dat$Site[1])
    
    return(smry)
}

out <- lapply(names(dirs_to_process), f, 
              dirs_to_process = dirs_to_process, out_dir = L1A)
```

## File summary

```{r summary}
#| echo: false
#| output: asis
if(errors) {
    cat("### WARNING: ", errors, " file read/write error(s)\n")
    log_warning(paste("File read/write error(s)", params$html_outfile), 
                logfile = params$logfile)
}
```

## Output summary

```{r output_summary_table}
out_df <- do.call("rbind", out)
knitr::kable(out_df)
```

## Reproducibility

```{r reproducibility}
sessionInfo()
```
